{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import nltk\n",
    "import random \n",
    "from nltk.corpus import movie_reviews \n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#listing positive review files and negative review files\n",
    "positive_review_files = [] # only positive review files\n",
    "negative_review_files = [] # only negative review files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileids = movie_reviews.fileids()\n",
    "for i in fileids:\n",
    "    filename = str(i)\n",
    "    filename = filename.split('/')\n",
    "    if filename[0] == 'neg':\n",
    "        negative_review_files.append(str(i))\n",
    "    elif filename[0] == 'pos':\n",
    "        positive_review_files.append(str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listing all te positive sentences from the files\n",
    "list_of_positive_sentences = [] # list of positive sentences\n",
    "# listing all the negative sentences from the files \n",
    "list_of_negative_sentences = [] # list of negative sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in positive_review_files:\n",
    "    words = movie_reviews.raw(i)\n",
    "    sentences = words.split('\\n')\n",
    "    for j in sentences:\n",
    "        list_of_positive_sentences.append(j)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in negative_review_files:\n",
    "    words = movie_reviews.raw(i)\n",
    "    sentences = words.split('\\n')\n",
    "    for j in sentences:\n",
    "        list_of_negative_sentences.append(j)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u\"films adapted from comic books have had plenty of success , whether they're about superheroes ( batman , superman , spawn ) , or geared toward kids ( casper ) or the arthouse crowd ( ghost world ) , but there's never really been a comic book like from hell before . \",\n",
       " u\"for starters , it was created by alan moore ( and eddie campbell ) , who brought the medium to a whole new level in the mid '80s with a 12-part series called the watchmen . \"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_positive_sentences[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is another source of data that is appended to the list of positive and negative sentences accordingly.\n",
    "with open('sentimenter.csv') as f:\n",
    "    r = csv.reader(f)\n",
    "    for row in r:\n",
    "        if row[1] == '0':\n",
    "            list_of_negative_sentences.append(row[3])\n",
    "        elif row[1] == '1':\n",
    "            list_of_positive_sentences.append(row[3])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('large/sentimentasset.csv') as f:\n",
    "    r = csv.reader(f)\n",
    "    for row in r:\n",
    "        if row[1] == '0':\n",
    "            list_of_negative_sentences.append(row[3])\n",
    "        elif row[3] == '1':\n",
    "            list_of_positive_sentences.append(row[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting words in the training set\n",
    "for i in list_of_negative_sentences:\n",
    "    X_train.append(i)\n",
    "    y_train.append('negative')\n",
    "for i in list_of_positive_sentences:\n",
    "    X_train.append(i)\n",
    "    y_train.append('positive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "list1 = []\n",
    "for i in STOPWORDS:\n",
    "    list1.append(i)\n",
    "STOPWORDS = list1\n",
    "list1 = []\n",
    "for i in range(len(STOPWORDS)):\n",
    "    if len(STOPWORDS[i]) < 2:\n",
    "        list1.append(STOPWORDS[i])\n",
    "STOPWORDS = list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Its time to cleanse the data\n",
    "REPLACE_BY_SPACE_RE = re.compile(r'[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile(r'[^0-9a-z #+_]')\n",
    "def text_prepare(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = text.lower()# lowercase text\n",
    "    text = re.sub(REPLACE_BY_SPACE_RE,\" \",text)# replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = re.sub(BAD_SYMBOLS_RE,\"\",text)# delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    temp = re.split(\" \",text)\n",
    "    for i in temp:\n",
    "        if i == \"\":\n",
    "            temp.remove(i)\n",
    "    a = ''\n",
    "    for i in temp:\n",
    "        if i not in STOPWORDS:\n",
    "            a = a + i + ' '\n",
    "    a = a[:len(a)-1]\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this block we have stored the cleansed data \n",
    "X_train = [text_prepare(x) for x in X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.csv','a+') as f:\n",
    "    w = csv.writer(f)\n",
    "    with open('label.csv','a+') as g:\n",
    "        v = csv.writer(g)\n",
    "        while len(x_train) != 0:\n",
    "            temp = random.choice(x_train)\n",
    "            idx = x_train.index(temp)\n",
    "            w.writerow([temp])\n",
    "            v.writerow([Y_train[idx]])\n",
    "            x_train.remove(temp)\n",
    "            Y_train.remove(Y_train[idx])\n",
    "            if len(x_train) % 100 ==0:\n",
    "                print len(x_train)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You can start creating your model from the saved data\n",
    "we will create a `positive-negative.pkl` as our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "with open('train.csv','r') as f:\n",
    "    w = csv.reader(f)\n",
    "    for i in w:\n",
    "        X_train.append(i)\n",
    "with open('label.csv','r') as f:\n",
    "    r = csv.reader(f)\n",
    "    for i in r:\n",
    "        y_train.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = X_train\n",
    "temper = y_train\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in temp:\n",
    "    X_train.append(i[0])\n",
    "for j in temper: \n",
    "    y_train.append(j[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['exams comin up',\n",
       " 'sweetangel69 doin good',\n",
       " 'sydneyfamous dont forget to tell your friends about the contest so they can get in for free']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### words count and reverse dictionary\n",
    "Here we are going to count the words  and assign the dictionaries to count them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of all tags from train corpus with their counts.\n",
    "tags_counts = {}\n",
    "\n",
    "labels = []\n",
    "for i in y_train:\n",
    "    labels.append(i)\n",
    "tags_counts = dict(Counter(labels))\n",
    "\n",
    "# Dictionary of all words from train corpus with their counts.\n",
    "words_counts = {}\n",
    "\n",
    "words = []\n",
    "for i in X_train:\n",
    "    for word in re.split(\" \",i):\n",
    "        words.append(word)\n",
    "words_counts = dict(Counter(words))\n",
    "for i in words_counts.keys():\n",
    "    if i == \"\":\n",
    "        words_counts.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('to', 117532), ('the', 111398), ('my', 65442), ('and', 63340), ('is', 50605), ('you', 47841), ('it', 47162), ('in', 45678), ('for', 42254), ('of', 40322), ('im', 36563), ('on', 33635), ('me', 32345), ('so', 31108), ('have', 30166), ('that', 29917), ('but', 28279), ('just', 24555), ('not', 24018), ('at', 23374)]\n"
     ]
    }
   ],
   "source": [
    "# most common words \n",
    "most_common_words = sorted(words_counts.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "print most_common_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the tf-idf vectorizer\n",
    "this is for the vectorising the words that we get.\n",
    "we will get a `positive-negative-tfidf` as a result of this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_features(X_train):\n",
    "    \n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(min_df=5, max_df=0.9, ngram_range=(1,2))####### YOUR CODE HERE #######\n",
    "    tfidf_vectorizer = tfidf_vectorizer.fit(X_train)\n",
    "    X_train = tfidf_vectorizer.transform(X_train)\n",
    "    \n",
    "    return tfidf_vectorizer,X_train, tfidf_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer, X_train_tfidf, tfidf_vocab = tfidf_features(X_train)\n",
    "tfidf_reversed_vocab = {i:word for word,i in tfidf_vocab.items()}\n",
    "#basically tfidf_reversed_vocab is a dictionary that stores the indexes to the vocabs created by the TfidfVectorizer\n",
    "# I learned it from the video tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tfidf_vectorizer,open('positive-negative-tfidf.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for i in y_train:\n",
    "    ls = []\n",
    "    ls.append(i)\n",
    "    temp.append(ls)\n",
    "y_train = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer(classes=sorted(tags_counts.keys()))\n",
    "y_train = mlb.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(mlb,open('multilabel-binarizer.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(X_train, y_train):\n",
    "    \"\"\"\n",
    "      X_train, y_train â€” training data\n",
    "      \n",
    "      return: trained classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    #Create and fit LogisticRegression wraped into OneVsRestClassifier.\n",
    "    classifier = OneVsRestClassifier(LogisticRegression(C=1.0))\n",
    "    classifier = classifier.fit(X_train,y_train)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_classifier = train_classifier(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tag_classifier, open('positive-negative.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now testing the model we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_output(tfidf_vectorizer,tag_classifier,mlb,test):\n",
    "    test1 = []\n",
    "    test1.append(test)\n",
    "    x_test_tfidf = tfidf_vectorizer.transform(test1)\n",
    "    test_predictions = tag_classifier.predict(x_test_tfidf)\n",
    "    test_pred_inversed = mlb.inverse_transform(test_predictions)\n",
    "    return test_pred_inversed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle_file(filename):\n",
    "    \"\"\"Returns the result of unpickling the file content.\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = unpickle_file('positive-negative-tfidf.pkl')\n",
    "tag_classifier = unpickle_file('positive-negative.pkl')\n",
    "multilabel_binarizer = unpickle_file('multilabel-binarizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter a data >> i am mad\n",
      "[('negative',)]\n"
     ]
    }
   ],
   "source": [
    "testing_data = raw_input('enter a data >> ')\n",
    "print generate_output(tfidf_vectorizer,tag_classifier,multilabel_binarizer,testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
